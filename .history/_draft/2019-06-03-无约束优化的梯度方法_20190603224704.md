---
layout:     post
title:      无约束优化的梯度方法
subtitle:   Gradient Methods for Unconstrained Problems
date:       2018-11-28
author:     shellyyz
# description: some common commands in Git.
# header-img: img/post-bg-ios9-web.jpg -->
catalog: 	 true
comments: true
tags:
    - 凸优化
    - 优化方法
---
{% include head.html %}
# 无约束优化的梯度方法

[TOC]

本文主要参照Princeton Prof. Yunxin Chen的Slides以及课堂笔记做的总结，聊一下无约束问题的梯度优化方法。既然提到了梯度，那么讨论的目标函数一定是可微的(可以用任意点处的的切平面来近似该点处的函数曲面)。后面有时间还会写一下约束优化问题的梯度方法，以及针对不可微目标函数的次梯度方法。

先来介绍几个基本概念：下降方向，迭代下降算法，以及大名鼎鼎的梯度下降。

* 下降方向 decent direction: 对于函数在$x$处沿$d$的方向导数$f'(x;d):=lim_{\tau \rightarrow \infty} \frac{f(x+\tau d)-f(x)}{\tau} = \nabla f(x)^{\top} d$，如果$d$满足方向导数$f'(x;d)=\nabla f(x)^{\top} d <0$, 那么称$d$为下降方向。
* 迭代下降算法 iterative descent algorithms： 从点$x^0$开始，构造序列${x^t}$，使得
  $$f(x^{t+1})<f(x^{t}), t=0,1,...$$ 
    在每次迭代中，寻找在当前 $x^t$ 点的下降方向 $d^t$ , 其中 $\eta_t>0$ 为步长。
* 梯度下降 Gradient Descent: $x^{t+1} = x^{t}-\eta_{t} \nabla f(x^t)$, 下降方向 $d^t=-\nabla f(x^t)$, 即最陡峭的下降方向，由Cauchy-Schwarz不等式 
  $$arg min_{d:||d||_2\le1}f'(x;d)=arg min_{||d||_2\le1}\nabla f(x)^{\top}d=-\frac{\nabla f(x)}{||\nabla f(x)||_2}$$

下面由特殊到一般，讨论一下各类无约束优化问题的下降算法及其收敛速度。

  ## 二次优化问题 Quadratic minimization problems
$$\min_x f(x):=\frac{1}{2} (x-x^*)^{\top}Q(x-x^*)$$
其中$Q\succ0, Q\in \mathbb{R}^{n\times n}$. $\nabla f(x)=Q(x-x^*)$. 
### 常数步长的收敛性
当每次迭代步长为固定值，步长的最优选择为$\eta_t = \eta=\frac{2}{\lambda_1(Q)+\lambda_n(Q)}$, 此时有
$$||x^t-x^*||_2\le \left(\frac{\lambda_1(Q)-\lambda_n(Q)}{\lambda_1(Q)+\lambda_n(Q)}\right)^t ||x^0-x^*||_2$$
* 下面的证明会看到，选取步长使得$|1-\eta \lambda_n(Q)|=|1-\eta \lambda_1(Q)|$
* 收敛速度取决于$Q$的 condition number $\frac{\lambda_1(Q)}{\lambda_n(Q)}$，即$\frac{\max_x{\lambda_1(\nabla^2 f(x))}}{\min_x{\lambda_n(\nabla^2 f(x))}}$。
* 该收敛速度称为线性收敛 linear convergence 或几何收敛 geometric convergence. 这是因为log(1/误差)和迭代次数 t 成线性关系，系数是log(1/$\alpha$), $\alpha$为$\frac{\lambda_1(Q)-\lambda_n(Q)}{\lambda_1(Q)+\lambda_n(Q)}$。
* 
